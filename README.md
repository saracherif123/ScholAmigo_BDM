# üéì ScholAmigo_BDM


**ScholAmigo_BDM** is a data engineering platform designed for the **Big Data Management** course at **UPC**. It orchestrates the collection, processing, and management of data related to international scholarship programs, leveraging modern data tools and cloud infrastructure.

---

## üåê Live Demo
[Visit the ScholAmigo Website](https://scholamigo-website.lovable.app)

---

## üöÄ Features
- Automated staging and organization of scholarship data
- Processed data pushed to AWS S3
- Workflow orchestration with Apache Airflow DAGs
- Secure AWS connectivity via GitHub Actions
- Clean `.env` configuration for easy local development

---

## üèóÔ∏è System Architecture

![System Architecture](./ScholAmigo%20-%20Architecture%20Design%20-%20P2.png)

---

## ‚öôÔ∏è Technologies Used
- **Orchestration & Cloud:**
  - Apache Airflow 2.8.1 (via Docker Compose)
  - AWS S3 (cloud data storage)
  - AWS CLI (command-line AWS operations)
  - Docker & Docker Compose
  - Git & GitHub Actions (CI/CD & cloud automation)

- **Data Processing & Analysis:**
  - Python & Boto3
  - Pandas, NumPy
  - Jupyter Notebooks (exploratory analysis)
  - Parquet (efficient data storage format)
  - JSON/JSONL (data interchange formats)
  - SQL (data querying and transformation)
  - Shell scripting (automation scripts)

- **Machine Learning & Vector Search:**
  - FAISS (vector database for similarity search)

- **PDF & Data Extraction:**
  - PyPDF2 or pdfminer (PDF extraction for resume processing)

- **Visualization:**
  - Matplotlib, Seaborn (data visualization)

---

## üóÇÔ∏è Project Structure
```
ScholAmigo_BDM/
‚îú‚îÄ‚îÄ airflow/                  # Airflow orchestration (DAGs, Docker, logs)
‚îÇ   ‚îú‚îÄ‚îÄ dags/                 # Airflow DAG scripts
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yaml   # Airflow Docker Compose config
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile            # Airflow Dockerfile
‚îÇ   ‚îî‚îÄ‚îÄ logs/                 # Airflow logs
‚îÇ
‚îú‚îÄ‚îÄ exploitation_zone/        # Data processing, matching, and vector DB
‚îÇ   ‚îú‚îÄ‚îÄ formatters/           # Data formatting scripts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 00_Scholarships_formatter.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 01_Aggregator_formatter.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 02_student_alumni_graph_creation.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 03_graph_cypher_queries.py
‚îÇ   ‚îú‚îÄ‚îÄ scripts/              # SQL and export scripts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ export_scholarships.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Scholarships.sql
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ truncateAll.sql
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Exploitation_Sch
‚îÇ   ‚îú‚îÄ‚îÄ data/                 # Processed data (e.g., scholarships.json)
‚îÇ   ‚îú‚îÄ‚îÄ vector_database/      # FAISS index and metadata
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cv_index.faiss
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cv_metadata.json
‚îÇ   ‚îú‚îÄ‚îÄ scholarship_matcher.py
‚îÇ   ‚îú‚îÄ‚îÄ setup_db.sh
‚îÇ   ‚îú‚îÄ‚îÄ setup.sh
‚îÇ   ‚îî‚îÄ‚îÄ init_vector_db.py
‚îÇ
‚îú‚îÄ‚îÄ landing_zone/             # Data collection (raw data, collectors)
‚îÇ   ‚îú‚îÄ‚îÄ collectors/           # Data collector scripts and notebooks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ erasmus_collector.py/.ipynb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ uk_collector.py/.ipynb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ users_collector.py/.ipynb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scholarship_collector.py/.ipynb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ resume_collector.py/.ipynb
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ LinkedinProfiles_collector.py/.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ data/                 # Raw data (scholarships, resumes, etc.)
‚îÇ       ‚îú‚îÄ‚îÄ aggregator_data/
‚îÇ       ‚îú‚îÄ‚îÄ erasmus_data/
‚îÇ       ‚îú‚îÄ‚îÄ erasmus_linkedin_profiles/
‚îÇ       ‚îú‚îÄ‚îÄ resume_data/
‚îÇ       ‚îú‚îÄ‚îÄ uk_data/
‚îÇ       ‚îî‚îÄ‚îÄ users_data/
‚îÇ
‚îú‚îÄ‚îÄ trusted_zone/             # Cleaned/processed data and formatters
‚îÇ   ‚îú‚îÄ‚îÄ formatters/           # Data cleaning and transformation scripts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aggregator_processor.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ student_alumni_cleaner.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ UK_Scholarships_Extract_Fields.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ UK_Scholarships_clean_load.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Erasmus_Extract_Fields.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Erasmus_clean_load.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cv_extract_pdfs.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cv_upload_to_s3.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cv_transformer.py
‚îÇ   ‚îî‚îÄ‚îÄ data/                 # Cleaned data (JSON, Parquet, etc.)
‚îÇ       ‚îú‚îÄ‚îÄ aggregator_data/
‚îÇ       ‚îú‚îÄ‚îÄ erasmus_data/
‚îÇ       ‚îú‚îÄ‚îÄ graph_data/
‚îÇ       ‚îú‚îÄ‚îÄ resume_data/
‚îÇ       ‚îî‚îÄ‚îÄ uk_data/
‚îÇ
‚îú‚îÄ‚îÄ metadata/                 # Schemas, metadata, and scripts
‚îÇ   ‚îú‚îÄ‚îÄ schemas/              # JSON schemas for data validation
‚îÇ   ‚îú‚îÄ‚îÄ scripts/              # Metadata-related scripts
‚îÇ   ‚îú‚îÄ‚îÄ description.md        # Project metadata description
‚îÇ   ‚îú‚îÄ‚îÄ registry.yaml         # Data registry
‚îÇ   ‚îî‚îÄ‚îÄ sources.json          # Data sources
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îú‚îÄ‚îÄ README.md                 # Project documentation
‚îú‚îÄ‚îÄ ScholAmigo - Architecture Design - P2.png # System architecture diagram
‚îî‚îÄ‚îÄ BDM_P2_Report.pdf         # Project report
```

---

## üõ†Ô∏è Setup Instructions (Local)

> **Requirements:** Docker, Docker Compose, (optional: AWS CLI)

1. **Clone the Repository**
   ```bash
   git clone https://github.com/YOUR_USERNAME/ScholAmigo_BDM.git
   cd ScholAmigo_BDM/airflow
   ```

2. **Start Airflow Locally**
   ```bash
   docker compose up
   ```

3. **Access the Airflow UI**
   - Open your browser: [http://localhost:8080](http://localhost:8080)
   - Login (default):
     ```
     Username: airflow
     Password: airflow
     ```

---

## ‚ñ∂Ô∏è Usage

- **Trigger a DAG Manually:**
  ```bash
  docker compose exec airflow-cli airflow dags list
  docker compose exec airflow-cli airflow dags trigger -d <dag_id>
  ```
  Replace `<dag_id>` with your actual DAG name.

---

## üß™ Testing (GitHub Actions)

A workflow is available to test AWS S3 integration:

1. Go to the [Actions tab](../../actions) of this repository.
2. Click **"Test AWS Access"** workflow.
3. Click **"Run workflow"** ‚Üí Select branch ‚Üí Click **"Run"**.

This will:
- Authenticate to AWS using GitHub Secrets
- List S3 buckets (visible in the logs)

> AWS credentials are managed securely using GitHub Secrets (`AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_DEFAULT_REGION`).

---

## ü§ù Contributing

Contributions are welcome! Please open an issue or submit a pull request for improvements, bug fixes, or new features.