import os
import streamlit as st
from dotenv import load_dotenv
from PIL import Image
import google.generativeai as genai
from pdf2image import convert_from_path
import pytesseract
import pdfplumber
import numpy as np
from sentence_transformers import SentenceTransformer
import json
import pickle
import psycopg2
from psycopg2.extras import RealDictCursor
import faiss
from typing import List, Dict, Any

# Load environment variables
load_dotenv()

# Configure Google Gemini AI
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

# Database connection
def get_db_connection():
    """Create a connection to the PostgreSQL database"""
    try:
        print(f"Connecting to database with settings:")
        print(f"Host: {os.getenv('DB_HOST')}")
        print(f"Database: {os.getenv('DB_NAME')}")
        print(f"User: {os.getenv('DB_USER')}")
        
        conn = psycopg2.connect(
            host=os.getenv("DB_HOST"),
            database=os.getenv("DB_NAME"),
            user=os.getenv("DB_USER"),
            password=os.getenv("DB_PASSWORD")
        )
        print("Database connection successful!")
        return conn
    except Exception as e:
        print(f"Error connecting to database: {e}")
        raise e

# Initialize sentence transformer model
@st.cache_resource
def load_embedding_model():
    return SentenceTransformer('all-MiniLM-L6-v2')

# Load FAISS index and metadata
@st.cache_resource
def load_vector_db():
    """Load FAISS index and metadata"""
    # Get the absolute path to the vector_database directory
    current_dir = os.path.dirname(os.path.abspath(__file__))
    index_path = os.path.join(current_dir, "vector_database", "cv_index.faiss")
    metadata_path = os.path.join(current_dir, "vector_database", "cv_metadata.json")
    
    # Load the index and metadata
    index = faiss.read_index(index_path)
    with open(metadata_path, 'r') as f:
        metadata = json.load(f)
    
    return index, metadata

# Function to extract text from PDF
def extract_text_from_pdf(pdf_path):
    """Extract text from PDF using multiple methods for better accuracy"""
    text = ""
    
    # Try pdfplumber first
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page in pdf.pages:
                text += page.extract_text() or ""
    except Exception as e:
        print(f"Error with pdfplumber: {e}")
    
    # If text is empty or too short, try OCR
    if len(text.strip()) < 100:
        try:
            # Convert PDF to images
            images = convert_from_path(pdf_path)
            
            # Extract text from each image
            for image in images:
                text += pytesseract.image_to_string(image) + "\n"
        except Exception as e:
            print(f"Error with OCR: {e}")
    
    return text

# Function to analyze CV
def analyze_cv(resume_text):
    model = genai.GenerativeModel("gemini-1.5-flash")
    
    prompt = f"""
    You are an expert CV reviewer. Analyze the following CV and provide a detailed review in the following format:
    
    1. Overall Score (0-100)
    2. Strengths
    3. Areas for Improvement
    4. Key Skills Identified
    5. Education Level
    6. Years of Experience
    7. Industry/Field
    8. Recommendations for Enhancement
    
    CV:
    {resume_text}
    """
    
    try:
        response = model.generate_content(prompt)
        if not response or not response.text:
            return "No response from Gemini model"
        return response.text
    except Exception as e:
        return f"Failed to get analysis: {str(e)}"

# Function to match CV with scholarship
def match_cv_with_scholarship(resume_text, scholarship_name):
    model = genai.GenerativeModel("gemini-1.5-flash")
    
    prompt = f"""
    You are an expert scholarship advisor. Analyze how well the following CV matches with the scholarship: {scholarship_name}
    
    CV:
    {resume_text}
    
    Provide a detailed analysis in the following format:
    1. Match Score (0-100)
    2. Key Matching Points
    3. Missing Requirements
    4. Recommendations to Improve Match
    5. Overall Assessment
    
    Format your response as a JSON object with the following structure:
    {{
        "match_score": 85,
        "matching_points": ["Point 1", "Point 2", ...],
        "missing_requirements": ["Requirement 1", "Requirement 2", ...],
        "improvement_recommendations": ["Recommendation 1", "Recommendation 2", ...],
        "overall_assessment": "Detailed assessment text"
    }}
    """
    
    response = model.generate_content(prompt)
    try:
        return json.loads(response.text)
    except:
        return {"error": "Failed to parse match analysis", "raw_text": response.text}

# Function to find similar CVs
def find_similar_cvs(resume_text, model, index, metadata, top_k=5):
    # Create embedding for resume
    resume_embedding = model.encode([resume_text])[0]
    query_embedding = resume_embedding.reshape(1, -1).astype('float32')
    
    # Search in FAISS index
    distances, indices = index.search(query_embedding, top_k)
    
    # Format results
    similar_cvs = []
    for i, idx in enumerate(indices[0]):
        if idx != -1:  # FAISS returns -1 for empty slots
            similar_cvs.append({
                'cv': metadata[str(idx)],
                'similarity_score': float(1 - distances[0][i])  # Convert distance to similarity score
            })
    
    return similar_cvs

def fetch_scholarships_from_db() -> List[Dict[str, Any]]:
    """Fetch scholarships from PostgreSQL database"""
    try:
        print("Attempting to fetch scholarships from database...")
        conn = get_db_connection()
        cur = conn.cursor(cursor_factory=RealDictCursor)
        
        # First, let's check if we can get basic scholarship data
        print("Checking basic scholarship data...")
        cur.execute("SELECT COUNT(*) FROM scholarships_common")
        common_count = cur.fetchone()['count']
        print(f"Found {common_count} scholarships in scholarships_common")
        
        cur.execute("SELECT COUNT(*) FROM scholarships_erasmus")
        erasmus_count = cur.fetchone()['count']
        print(f"Found {erasmus_count} scholarships in scholarships_erasmus")
        
        # Query to get all scholarship information with more lenient JOINs
        query = """
        SELECT 
            sc.key,
            sc.funding,
            sc.level,
            sc.required_level,
            sc.intake,
            sc.status,
            se.name,
            se.url,
            se.description,
            se.duration,
            se.scholarship_available_for_current_intake,
            se.required_documents,
            se.course_topics,
            se.requirements,
            se.programme_tuition_cov,
            se.programme_monthly_allowance,
            se.programme_installation_costs,
            se.partner_tuition_cov,
            se.partner_monthly_allowance,
            se.partner_installation_costs,
            COALESCE(array_agg(DISTINCT c.name) FILTER (WHERE c.name IS NOT NULL), ARRAY[]::text[]) as countries,
            COALESCE(array_agg(DISTINCT f.field) FILTER (WHERE f.field IS NOT NULL), ARRAY[]::text[]) as fields_of_study,
            COALESCE(array_agg(DISTINCT u.university) FILTER (WHERE u.university IS NOT NULL), ARRAY[]::text[]) as universities,
            COALESCE(array_agg(DISTINCT d.date::text) FILTER (WHERE d.date IS NOT NULL), ARRAY[]::text[]) as deadlines
        FROM scholarships_common sc
        LEFT JOIN scholarships_erasmus se ON sc.key = se.key
        LEFT JOIN origin_countries_sch ocs ON sc.key = ocs.sch_key
        LEFT JOIN countries c ON ocs.country_key = c.key
        LEFT JOIN scholarships_fields sf ON sc.key = sf.sch_key
        LEFT JOIN fields_of_study f ON sf.field_key = f.key
        LEFT JOIN sch_uni su ON sc.key = su.sch_key
        LEFT JOIN universities u ON su.uni_key = u.key
        LEFT JOIN deadlines d ON sc.key = d.sch_key
        GROUP BY sc.key, se.key, se.name, se.url, se.description, se.duration, 
                 se.scholarship_available_for_current_intake, se.required_documents,
                 se.course_topics, se.requirements, se.programme_tuition_cov,
                 se.programme_monthly_allowance, se.programme_installation_costs,
                 se.partner_tuition_cov, se.partner_monthly_allowance, se.partner_installation_costs
        """
        
        print("Executing main query...")
        cur.execute(query)
        scholarships = cur.fetchall()
        print(f"Query returned {len(scholarships)} scholarships")
        
        if not scholarships:
            print("Warning: No scholarships found in the database")
            return []
        
        # Convert to list of dictionaries
        scholarships_list = []
        for sch in scholarships:
            scholarship = dict(sch)
            # Convert any None values to empty strings or lists
            for key, value in scholarship.items():
                if value is None:
                    if isinstance(scholarship[key], list):
                        scholarship[key] = []
                    else:
                        scholarship[key] = ""
            scholarships_list.append(scholarship)
        
        print(f"Successfully processed {len(scholarships_list)} scholarships")
        cur.close()
        conn.close()
        
        return scholarships_list
    except Exception as e:
        print(f"Error fetching scholarships from database: {e}")
        return []

def extract_cv_sections(cv_text: str) -> Dict[str, str]:
    """Extract key sections from CV using Gemini AI"""
    model = genai.GenerativeModel("gemini-1.5-flash")
    
    prompt = f"""
    Extract the following sections from this CV and return them as a JSON object:
    1. Education (including degrees, institutions, and years)
    2. Skills (technical skills, soft skills, languages)
    3. Experience (work experience, internships, projects)
    4. Other (any other relevant information)
    
    CV:
    {cv_text}
    
    Return the response in this exact JSON format:
    {{
        "education": "extracted education text",
        "skills": "extracted skills text",
        "experience": "extracted experience text",
        "other": "extracted other text"
    }}
    """
    
    try:
        response = model.generate_content(prompt)
        sections = json.loads(response.text)
        return sections
    except Exception as e:
        print(f"Error extracting CV sections: {e}")
        # Return original text in all sections if extraction fails
        return {
            "education": cv_text,
            "skills": cv_text,
            "experience": cv_text,
            "other": cv_text
        }

def create_scholarship_embeddings(scholarships: List[Dict[str, Any]], model: SentenceTransformer) -> tuple:
    """Create embeddings for scholarships and build FAISS index"""
    if not scholarships:
        print("Error: No scholarships provided for embedding creation")
        return None, []
    
    # Prepare text for embedding
    scholarship_texts = []
    for i, scholarship in enumerate(scholarships):
        # Ensure all required fields exist
        required_fields = ['name', 'description', 'requirements', 'level', 'required_level', 
                         'funding', 'fields_of_study', 'countries', 'universities', 
                         'duration', 'course_topics', 'required_documents']
        
        # Check if all required fields exist
        missing_fields = [field for field in required_fields if field not in scholarship]
        if missing_fields:
            print(f"Warning: Scholarship {i+1} missing required fields: {missing_fields}")
            continue
            
        # Create a more comprehensive text for matching with weighted sections
        text = f"""
        Title: {scholarship['name']}
        Description: {scholarship['description']}
        
        # Education Requirements (Weight: 3)
        Level: {scholarship['level']}
        Required Level: {scholarship['required_level']}
        Fields of Study: {', '.join(scholarship['fields_of_study'])}
        
        # Skills and Experience Requirements (Weight: 2)
        Requirements: {scholarship['requirements']}
        Required Documents: {scholarship['required_documents']}
        
        # Additional Information (Weight: 1)
        Funding: {scholarship['funding']}
        Countries: {', '.join(scholarship['countries'])}
        Universities: {', '.join(scholarship['universities'])}
        Duration: {scholarship['duration']}
        Course Topics: {scholarship['course_topics']}
        """
        scholarship_texts.append(text)
        
        # Print sample of first scholarship for verification
        if i == 0:
            print("\n=== Sample Scholarship Text ===")
            print(text[:500])
            print("==============================\n")
    
    if not scholarship_texts:
        print("Error: No valid scholarship texts to create embeddings")
        return None, []
    
    # Create embeddings
    try:
        print(f"Creating embeddings for {len(scholarship_texts)} scholarships...")
        embeddings = model.encode(scholarship_texts)
        print(f"Successfully created embeddings for {len(scholarship_texts)} scholarships")
        
        # Create FAISS index
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatL2(dimension)
        index.add(embeddings.astype('float32'))
        
        return index, scholarships
    except Exception as e:
        print(f"Error creating embeddings: {e}")
        return None, []

def match_cv_with_scholarships(cv_text: str, model: SentenceTransformer, index: faiss.Index, 
                             scholarships: List[Dict[str, Any]], top_k: int = 5) -> List[Dict[str, Any]]:
    """Match CV with scholarships using semantic similarity with weighted sections"""
    # Extract CV sections
    cv_sections = extract_cv_sections(cv_text)
    
    print("\n=== CV Sections ===")
    for section, content in cv_sections.items():
        print(f"\n{section.upper()} (first 200 chars):")
        print(content[:200])
    print("==================\n")
    
    # Create weighted embeddings for each section
    section_weights = {
        "education": 3.0,  # Highest weight for education
        "skills": 2.0,     # Medium weight for skills
        "experience": 2.0, # Medium weight for experience
        "other": 1.0      # Lower weight for other information
    }
    
    # Create embeddings for each section
    section_embeddings = {}
    for section, content in cv_sections.items():
        section_embeddings[section] = model.encode([content])[0]
    
    # Combine embeddings with weights
    weighted_embedding = np.zeros_like(section_embeddings["education"])
    total_weight = sum(section_weights.values())
    
    for section, embedding in section_embeddings.items():
        weight = section_weights[section] / total_weight
        weighted_embedding += weight * embedding
    
    # Search in FAISS index
    weighted_embedding = weighted_embedding.reshape(1, -1).astype('float32')
    distances, indices = index.search(weighted_embedding, top_k)
    
    # Prepare results
    matches = []
    for i, idx in enumerate(indices[0]):
        if idx != -1:  # FAISS returns -1 for empty slots
            distance = distances[0][i]
            similarity_score = 100 * (1 / (1 + np.exp(distance - 1)))
            
            scholarship = scholarships[idx]
            
            # Print the scholarship text being matched
            print(f"\n=== Scholarship {i+1} Text Sample ===")
            print(f"Name: {scholarship['name']}")
            print(f"Description: {scholarship['description'][:200]}...")
            print(f"Fields: {', '.join(scholarship['fields_of_study'])}")
            print(f"Level: {scholarship['level']}")
            print(f"Similarity Score: {similarity_score:.1f}%")
            print("================================\n")
            
            matches.append({
                'scholarship': scholarship,
                'similarity_score': similarity_score
            })
    
    # Sort matches by similarity score in descending order
    matches.sort(key=lambda x: x['similarity_score'], reverse=True)
    return matches

def analyze_match(cv_text: str, scholarship: Dict[str, Any]) -> str:
    """Analyze how well the CV matches with a specific scholarship"""
    model = genai.GenerativeModel("gemini-1.5-flash")
    
    prompt = f"""
    You are an expert scholarship advisor. Analyze how well the following CV matches with this scholarship:
    
    Scholarship Details:
    Title: {scholarship['name']}
    Description: {scholarship['description']}
    Level: {scholarship['level']}
    Required Level: {scholarship['required_level']}
    Funding: {scholarship['funding']}
    Fields of Study: {', '.join(scholarship['fields_of_study'])}
    Countries: {', '.join(scholarship['countries'])}
    Universities: {', '.join(scholarship['universities'])}
    Requirements: {scholarship['requirements']}
    Required Documents: {scholarship['required_documents']}
    
    CV:
    {cv_text}
    
    Provide a detailed analysis in the following format:
    1. Match Score (0-100)
    2. Key Matching Points
    3. Missing Requirements
    4. Recommendations to Improve Match
    5. Overall Assessment
    
    Format your response as a JSON object with the following structure:
    {{
        "match_score": 85,
        "matching_points": ["Point 1", "Point 2", ...],
        "missing_requirements": ["Requirement 1", "Requirement 2", ...],
        "improvement_recommendations": ["Recommendation 1", "Recommendation 2", ...],
        "overall_assessment": "Detailed assessment text"
    }}
    
    Important: Return ONLY the JSON object, without any markdown formatting or code blocks.
    """
    
    try:
        response = model.generate_content(prompt)
        # Clean the response text to remove any markdown formatting
        cleaned_text = response.text.strip()
        if cleaned_text.startswith('```json'):
            cleaned_text = cleaned_text[7:]
        if cleaned_text.endswith('```'):
            cleaned_text = cleaned_text[:-3]
        cleaned_text = cleaned_text.strip()
        
        # Parse the cleaned JSON
        analysis = json.loads(cleaned_text)
        
        # Format the analysis for display
        formatted_analysis = f"""
        ## Match Score: {analysis['match_score']}%

        ### Key Matching Points
        {chr(10).join('‚Ä¢ ' + point for point in analysis['matching_points'])}

        ### Missing Requirements
        {chr(10).join('‚Ä¢ ' + req for req in analysis['missing_requirements'])}

        ### Recommendations to Improve Match
        {chr(10).join('‚Ä¢ ' + rec for rec in analysis['improvement_recommendations'])}

        ### Overall Assessment
        {analysis['overall_assessment']}
        """
        
        return formatted_analysis
    except Exception as e:
        print(f"Error parsing analysis: {e}")
        print(f"Raw response: {response.text}")
        return f"Failed to parse match analysis. Error: {str(e)}\n\nRaw response:\n{response.text}"

# Streamlit app
st.set_page_config(page_title="Scholarship CV Matcher", layout="wide")

# Custom CSS for centering main elements but keeping content left-aligned
st.markdown("""
    <style>
        .stApp {
            max-width: 1200px;
            margin: 0 auto;
        }
        .stButton>button {
            margin: 0 auto;
            display: block;
        }
        .stTabs [data-baseweb="tab-list"] {
            justify-content: center;
        }
        .stFileUploader {
            text-align: center;
        }
        /* Keep content left-aligned */
        .stMarkdown p, .stMarkdown li, .stMarkdown ul {
            text-align: left !important;
        }
        .stExpander {
            text-align: left !important;
        }
        .stAlert {
            text-align: left !important;
        }
    </style>
""", unsafe_allow_html=True)

# Title and Description (centered)
st.markdown("""
    <div style='text-align: center;'>
        <h1>üéì Scholarship CV Matcher</h1>
        <p>Upload your CV and find matching scholarships from our database!</p>
    </div>
""", unsafe_allow_html=True)

# Load models and data
embedding_model = load_embedding_model()
faiss_index, cv_metadata = load_vector_db()

# Fetch scholarships and create embeddings
with st.spinner("Loading scholarships..."):
    scholarships = fetch_scholarships_from_db()
    if not scholarships:
        st.error("No scholarships found in the database. Please check your database connection and data.")
        st.stop()
    
    scholarship_index, scholarships = create_scholarship_embeddings(scholarships, embedding_model)
    if scholarship_index is None:
        st.error("Failed to create scholarship embeddings. Please check the scholarship data format.")
        st.stop()

# File upload (centered)
st.markdown('<div style="text-align: center;">', unsafe_allow_html=True)
uploaded_file = st.file_uploader("üìÑ Upload your CV (PDF)", type=["pdf"])
st.markdown('</div>', unsafe_allow_html=True)

if uploaded_file is not None:
    # Save uploaded file
    with open("uploaded_cv.pdf", "wb") as f:
        f.write(uploaded_file.getbuffer())
    
    # Create tabs
    tab1, tab2, tab3 = st.tabs(["üìù Resume Analysis", "üéØ Scholarship Matches", "üìä Detailed Analysis"])
    
    with tab1:
        if st.button("üîç Analyze Resume"):
            with st.spinner("Analyzing resume... ‚è≥"):
                try:
                    # Analyze resume
                    analysis = analyze_cv(extract_text_from_pdf("uploaded_cv.pdf"))
                    st.success("‚úÖ Analysis complete!")
                    st.write(analysis)
                except Exception as e:
                    st.error(f"‚ùå Analysis failed: {str(e)}")
                    st.write("Please try again or check if the CV was properly uploaded.")

    with tab2:
        if st.button("üîç Find Matching Scholarships"):
            with st.spinner("Finding matching scholarships..."):
                # Extract text from CV
                resume_text = extract_text_from_pdf("uploaded_cv.pdf")
                
                # Find matches
                matches = match_cv_with_scholarships(resume_text, embedding_model, scholarship_index, scholarships)
                
                for i, match in enumerate(matches, 1):
                    scholarship = match['scholarship']
                    similarity = match['similarity_score']
                    
                    with st.expander(f"{i}. {scholarship['name']} (Match: {similarity:.1f}%)"):
                        # Description
                        if scholarship['description']:
                            st.write("**Description:**", scholarship['description'])
                        
                        # Requirements
                        if scholarship['requirements']:
                            st.write("**Requirements:**")
                            try:
                                # Try to parse as JSON for better formatting
                                requirements = json.loads(scholarship['requirements'])
                                for key, value in requirements.items():
                                    if value and value != "N/A":
                                        if key == "language_proficiency":
                                            st.write("**Language Requirements:**")
                                            try:
                                                # Handle the nested structure
                                                if isinstance(value, str):
                                                    lang_data = json.loads(value)
                                                else:
                                                    lang_data = value
                                                
                                                # Process each language requirement
                                                for lang_req in lang_data:
                                                    if isinstance(lang_req, list) and len(lang_req) >= 4:
                                                        # Get language and level
                                                        language = lang_req[2]
                                                        level = lang_req[3]
                                                        st.write(f"**{language} ({level})**")
                                                        
                                                        # Get test scores with smaller font
                                                        if isinstance(lang_req[0], list):
                                                            st.markdown('<div style="font-size: 0.9em;">', unsafe_allow_html=True)
                                                            for test in lang_req[0]:
                                                                if isinstance(test, list) and len(test) >= 2:
                                                                    st.markdown(f"‚Ä¢ {test[1]}: {test[0]}", unsafe_allow_html=True)
                                                        
                                                        # Get additional notes with smaller font
                                                        if len(lang_req) > 1 and lang_req[1]:
                                                            st.markdown(f"‚Ä¢ Note: {lang_req[1]}", unsafe_allow_html=True)
                                                            st.markdown('</div>', unsafe_allow_html=True)
                                            except Exception as e:
                                                st.write(f"Error parsing language requirements: {str(e)}")
                                                st.write(value)
                                        else:
                                            # Format the key nicely
                                            formatted_key = key.replace('_', ' ').title()
                                            if key == 'requirements_more_details_link':
                                                st.write(f"**{formatted_key}:** [Click here]({value})")
                                            else:
                                                st.write(f"**{formatted_key}:** {value}")
                            except:
                                st.write(scholarship['requirements'])
                        
                        # Field of Study
                        if scholarship['fields_of_study']:
                            st.write("**Fields of Study:**", ", ".join(scholarship['fields_of_study']))
                        
                        # Level
                        if scholarship['level']:
                            st.write("**Level of Study:**", scholarship['level'])
                        
                        # Countries
                        if scholarship['countries']:
                            st.write("**Available Countries:**", ", ".join(scholarship['countries']))
                        
                        # Deadlines
                        if scholarship['deadlines']:
                            st.write("**Application Deadlines:**", ", ".join(scholarship['deadlines']))
                        
                        # Funding
                        if scholarship['funding']:
                            st.write("**Funding:**", scholarship['funding'])
                        
                        # URL
                        if scholarship['url']:
                            st.write("**More Information:**", f"[Click here]({scholarship['url']})")
    
    with tab3:
        if st.button("üìä Analyze Top Match"):
            with st.spinner("Analyzing best match..."):
                try:
                    # Extract text from CV first
                    resume_text = extract_text_from_pdf("uploaded_cv.pdf")
                    
                    # Find matches
                    matches = match_cv_with_scholarships(resume_text, embedding_model, scholarship_index, scholarships, top_k=1)
                    if matches:
                        best_match = matches[0]
                        st.write("Analyzing match with:", best_match['scholarship']['name'])
                        analysis = analyze_match(resume_text, best_match['scholarship'])
                        st.success("‚úÖ Analysis complete!")
                        st.write(analysis)
                    else:
                        st.warning("No matching scholarships found.")
                except Exception as e:
                    st.error(f"‚ùå Analysis failed: {str(e)}")
                    st.write("Please try again or check if the CV was properly uploaded.")

# Footer
st.markdown("---")
st.markdown(
    """
    <style>
        .footer {
            position: fixed;
            bottom: 0;
            left: 0;
            width: 100%;
            text-align: center;
            padding: 10px;
            font-size: 14px;
        }
    </style>
    <div class="footer">
        üöÄ Powered by Streamlit and <a href="https://console.cloud.google.com/marketplace/product/google/gemini-ai" target="_blank">Google Gemini AI</a>
    </div>
    """,
    unsafe_allow_html=True) 