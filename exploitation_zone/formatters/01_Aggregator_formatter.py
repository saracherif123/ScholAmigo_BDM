import os
from pyspark.sql import SparkSession
import psycopg2
from psycopg2.extras import execute_values
import boto3
from datetime import datetime
import google.generativeai as genai
import json

aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID')
aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY')

# Configure Gemini API
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')


genai.configure(api_key=GOOGLE_API_KEY)
model = genai.GenerativeModel("gemini-1.5-flash-latest")


# Initialize Spark Session
spark = SparkSession.builder \
    .appName("ScholarshipDataProcessor") \
    .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider") \
    .config("spark.hadoop.fs.s3a.access.key", aws_access_key_id) \
    .config("spark.hadoop.fs.s3a.secret.key", aws_secret_access_key) \
    .config("spark.sql.jsonGenerator.ignoreNullFields", "false") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

# Configuration
S3_BUCKET = "scholamigo"
S3_PREFIX = "trusted_zone_data/aggregator_data/"
S3_KEY = f"s3a://{S3_BUCKET}/{S3_PREFIX}aggregator_processed.json"

POSTGRES_CONFIG = {
    "host": "localhost",
    "database": "ScholAmigo",
    "user": "marwasulaiman", #change to your username
    "port": 5432
}

STANDARD_FIELDS = {
    0: "Generic programmes and qualifications",
    1: "Education",
    2: "Arts and humanities",
    3: "Social sciences, journalism and information",
    4: "Business, administration and law",
    5: "Natural sciences, mathematics and statistics",
    6: "Information and Communication Technologies",
    7: "Engineering, manufacturing and construction",
    8: "Agriculture, forestry, fisheries and veterinary",
    9: "Health and welfare",
    10: "Services"
}

def get_field_mapping():
    return {v: k for k, v in STANDARD_FIELDS.items()}

def standardize_field_with_gemini(field_str):
    """Standardize field of study using Gemini API"""
    if not field_str or field_str.lower() == "unrestricted":
        return [STANDARD_FIELDS[0]]
    
    prompt = f"""
    Map this field of study to one of these standard categories: {STANDARD_FIELDS}
    Input field: "{field_str}"
    Return only the standard category name that best matches.
    """
    
    try:
        response = model.generate_content(prompt)
        result = response.text.strip()
        
        # Find the best matching standard field
        for field in STANDARD_FIELDS.values():
            if field.lower() in result.lower():
                return [field]
        return [STANDARD_FIELDS[0]]
    except Exception as e:
        print(f"Gemini API error: {e}")
        return [STANDARD_FIELDS[0]]

def determine_funding_type(scholarship_type):
    """Determine funding type from scholarship_type field"""
    if not scholarship_type:
        return "Unknown"
    
    scholarship_type = scholarship_type.lower()
    if "fully" in scholarship_type or "full" in scholarship_type:
        return "Full"
    elif "partial" in scholarship_type:
        return "Partial"
    else:
        return "Unknown"

def determine_required_level(level):
    """Determine required education level based on scholarship level"""
    if not level:
        return "Unknown"
    
    level = level.lower()
    if "phd" in level or "doctoral" in level:
        return "Master"
    elif "master" in level or "graduate" in level:
        return "Bachelor"
    elif "bachelor" in level or "undergrad" in level:
        return "High School"
    return "Unknown"

def process_scholarship_data():
    """Main function to process scholarship data"""
    conn = psycopg2.connect(**POSTGRES_CONFIG)
    cursor = conn.cursor()
    print("âœ… Connected to PostgreSQL database!")
    
    try:
        # Read JSON data from S3
        df = spark.read.option("multiLine", "true") \
                      .option("mode", "PERMISSIVE") \
                      .json(S3_KEY)
        
        print(f"Successfully read {df.count()} records from S3")
        
        # Get field mapping
        field_mapping = get_field_mapping()
        
        # Collect all unique fields of study first
        all_fields = set()
        for row in df.collect():
            fields = row.asDict().get('field_of_study', [])
            for field in fields:
                if field and field.lower() != "unrestricted":
                    all_fields.add(field.strip())
        
        # Standardize all unique fields in one Gemini request
        standardized_fields = {}
        if all_fields:
            prompt = f"""
            For each of the following fields of study, map it to the most appropriate standard category from this list: 
            {list(STANDARD_FIELDS.values())}
            
            Return your response as a JSON object where keys are the original fields and values are the standardized categories.
            
            Fields to map:
            {list(all_fields)}
            """
            
            try:
                response = model.generate_content(prompt)
                result = response.text.strip()
                
                # Try to parse the JSON response
                try:
                    standardized_fields = json.loads(result)
                    print("Successfully standardized fields with Gemini")
                except json.JSONDecodeError:
                    print("Failed to parse Gemini response as JSON")
                    # Fall back to individual processing
                    standardized_fields = {}
            except Exception as e:
                print(f"Gemini API error: {e}")
        
        processed_count = 0
        skipped_count = 0
        
        for row in df.collect():
            try:
                row_dict = row.asDict()
                
                # Extract basic information
                name = row_dict.get('scholarship_name', '').strip()
                if not name:
                    skipped_count += 1
                    continue
                
                description = row_dict.get('description', '')
                scholarship_type = row_dict.get('scholarship_type', '')
                level = row_dict.get('level', '')
                status = row_dict.get('status', '').lower()
                
                # Determine fields for scholarships_common
                funding = determine_funding_type(scholarship_type)
                required_level = determine_required_level(level)
                intake = "Fall"  # Default value
                status = "Open" if status == "active" else "Closed"

                if 'bachelor' in level.lower():
                    level = "Bachelor"
                elif 'master' in level.lower():
                    level = "Master"
                elif 'phd' in level.lower():
                    level = "PhD"
                elif 'postdoc' in level.lower():
                    level = "Postdoc"
                else:
                    level = "Unknown"
                
                # Insert into scholarships_common
                cursor.execute("""
                    INSERT INTO scholarships_common (
                        name, funding, level, required_level, intake, status
                    ) VALUES (%s, %s, %s, %s, %s, %s) RETURNING key
                """, (name, funding, level, required_level, intake, status))
                
                sch_key = cursor.fetchone()[0]
                
                # Process fields of study using pre-standardized fields or fallback
                fields = row_dict.get('field_of_study', [])
                for field in fields:
                    if not field or field.lower() == "unrestricted":
                        standardized_field = STANDARD_FIELDS[0]
                    else:
                        standardized_field = standardized_fields.get(field.strip(), STANDARD_FIELDS[0])
                    
                    if standardized_field in field_mapping:
                        cursor.execute("""
                            SELECT 1 FROM scholarships_fields 
                            WHERE sch_key = %s AND field_key = %s
                            LIMIT 1
                        """, (sch_key, field_mapping[standardized_field]))
                        
                        if not cursor.fetchone():
                            # Only insert if combination doesn't exist
                            cursor.execute("""
                                INSERT INTO scholarships_fields (sch_key, field_key)
                                VALUES (%s, %s)
                            """, (sch_key, field_mapping[standardized_field]))
                
                cursor.execute("""
                    INSERT INTO scholarships_other (
                        key, name, description
                    ) VALUES (%s, %s, %s) RETURNING key
                """, (sch_key, name, description))
                
                processed_count += 1
                if processed_count % 100 == 0:
                    conn.commit()
                    print(f"Processed {processed_count} scholarships...")
                    
            except Exception as e:
                conn.rollback()
                print(f"Error processing scholarship '{name}': {str(e)}")
                skipped_count += 1
                
        # Final commit
        conn.commit()
        print("\nProcessing complete!")
        print(f"Total processed: {processed_count}")
        print(f"Total skipped: {skipped_count}")
        
        # Verification
        cursor.execute("SELECT COUNT(*) FROM scholarships_common")
        total_scholarships = cursor.fetchone()[0]
        print(f"\nTotal scholarships in database: {total_scholarships}")
        
        cursor.execute("""
            SELECT sc.key, sc.name, sc.level, sc.funding 
            FROM scholarships_common sc
            ORDER BY sc.key DESC
            LIMIT 5
        """)
        
        print("\nSample scholarships:")
        for row in cursor.fetchall():
            print(f"ID: {row[0]}, Name: {row[1]}, Level: {row[2]}, Funding: {row[3]}")
        
    except Exception as e:
        print(f"Error during processing: {str(e)}")
        conn.rollback()
    finally:
        cursor.close()
        conn.close()
        spark.stop()
        print("All connections closed.")

if __name__ == "__main__":
    process_scholarship_data()